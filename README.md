# LLM Lies: Hallucinations are not Bugs, but Features as  Adversarial Examples

PyTorch implementation of
["**Hallucination-Attack**"](Under review as a conference paper at ICLR 2024ï¼‰

## Coming soon
